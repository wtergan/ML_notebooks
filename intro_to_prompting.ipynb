{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtergan/ML_notebooks/blob/main/intro_to_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple notebook for demonstrating and testing different prompts/ prompt techniques.\n",
        "\n",
        "Based on notebook by DAIR.AI | Elvis Saravia.\n",
        "  https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-lecture.ipynb\n",
        "\n",
        "Edited, rewritten, modified from the original for educational purposes."
      ],
      "metadata": {
        "id": "R6IpJ1gt_pLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPQ59tYY-8qV"
      },
      "outputs": [],
      "source": [
        "# Downloading, installation of libraries, dependencies. Usage of %%capture for suppression of installation outputs.\n",
        "%%capture\n",
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation of libraries, dependencies.\n",
        "import openai, os, IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv"
      ],
      "metadata": {
        "id": "ipjJmS6MAoQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading of environment variables (.env file can be used for OPENAI_API_KEY).\n",
        "load_dotenv(\"/content/OPENAI_API_KEY.env\")\n",
        "# API config.\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# For langchain.\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")"
      ],
      "metadata": {
        "id": "zSzsuK0oBWpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt basics."
      ],
      "metadata": {
        "id": "Q3xjutr4ObIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic functions used for text generation.\n",
        "\"\"\"\n",
        "Function that sets the parameters needed for text generation.\n",
        "@param model: (str) Name of the OpenAI model that will be used for text generation.\n",
        "@param temperature: (int) Parameter used to specify the randomness of the text generation.\n",
        "@param max_tokens: (int) The maximum number of tokens to generation.\n",
        "@param top_p: (int) Indicates the probabilty threshold for token sampling. 1 means all tokens are considered.\n",
        "@param frequency_penalty: (int) Penalty for repeating the same token or phrase in the gneerated text.\n",
        "@param presence_penalty: (int) Penalty for using tokens that have already been used in the generated text.\n",
        "\n",
        "@returns openai_params: (dict) Dictionary of the required parameters specified.\n",
        "\"\"\"\n",
        "def set_open_params(\n",
        "    model=\"text-davinci-003\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\" Set OpenAI parameters for usage. \"\"\"\n",
        "    openai_params={}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "\n",
        "\"\"\"\n",
        "Function that gets the prompt completion (text generation) from the OpenAI API.\n",
        "@param params: (dict) Dictionary of the required parameters.\n",
        "@param prompt: (str) String of text which the model will be responding to.\n",
        "\n",
        "@returns response: (dict) Dictionary of information of the response, including the\n",
        "                   'choices' key, which contains the text completion.\n",
        "\"\"\"\n",
        "def get_completion(params, prompt):\n",
        "  \"\"\" Get completion from the OpenAI API. \"\"\"\n",
        "  response = openai.Completion.create(\n",
        "      engine=params['model'],\n",
        "      prompt=prompt,\n",
        "      temperature=params['temperature'],\n",
        "      max_tokens=params['max_tokens'],\n",
        "      top_p=params['top_p'],\n",
        "      frequency_penalty = params['frequency_penalty'],\n",
        "      presence_penalty = params['presence_penalty'],\n",
        "  )\n",
        "  return response"
      ],
      "metadata": {
        "id": "PO1oyCNxD0Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic prompt example.\n",
        "params = set_open_params()\n",
        "prompt = \"The sky is\"\n",
        "response = get_completion(params, prompt)\n",
        "response['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "q9izUTa-JmTD",
        "outputId": "eb365f31-cad1-482a-df4e-d8f9842dbf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' blue\\n\\nThe sky is blue because molecules in the atmosphere scatter blue light from the sun more than they scatter red light. This is known as Rayleigh scattering.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can present the response better (in markdown), by using IPython.\n",
        "IPython.display.Markdown(response.choices[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "D2psXEkzNnhN",
        "outputId": "ac58ab28-ea73-452b-85f5-f5ea605ddb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " blue\n\nThe sky is blue in color due to the way the atmosphere scatters sunlight. Blue is scattered more than other wavelengths of light, so it is seen more often than other colors."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can change the parameters to get slightly different results. Lets attempt to change the temeperature.\n",
        "params = set_open_params(temperature=0)\n",
        "prompt = \"The sky is\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "YloAsEDeN7SP",
        "outputId": "4e578946-83ac-4cbd-f210-62cb6a1a75c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " blue\n\nThe sky is blue because of the way the atmosphere scatters sunlight. When sunlight passes through the atmosphere, the blue wavelengths are scattered more than the other colors, making the sky appear blue."
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Summarization.\n",
        "\n",
        "Summarization of prompts into quick and easy-to-read summaries. Can always specify to the model how to summarize (for example, summarize as if I am a child, or changing the length of the summary)."
      ],
      "metadata": {
        "id": "SvRcj19fOWkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing\n",
        "the bacteria or preventing them from reproducting, allowing the body's immune system to fight off the infection.\n",
        "Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered\n",
        "intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic\n",
        "resistance. Explain the above in one sentence:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "KjnUA-etOTzA",
        "outputId": "7c42f30a-2c7e-401b-a622-523a30a3182b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nAntibiotics are medications used to treat bacterial infections by either killing the bacteria or inhibiting their reproduction, but they are not effective against viral infections and should be used responsibly to avoid developing antibiotic resistance."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets instruct the model to explain the paragraph in one sentence like \"I am a 5 year old\"."
      ],
      "metadata": {
        "id": "adVimBuULkZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing\n",
        "the bacteria or preventing them from reproducting, allowing the body's immune system to fight off the infection.\n",
        "Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered\n",
        "intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic\n",
        "resistance. Explain the above in one sentence, and explain as if I am 5 years old:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ou87HHuKQM_1",
        "outputId": "bca809a1-ecce-4901-ad9f-3467b4f2a573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nAntibiotics are special medicines that help your body fight off bad bacteria, but they won't work against colds or other virus germs."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Extraction\n",
        "\n",
        "Retrieval of information from the prompt provided."
      ],
      "metadata": {
        "id": "ALpUfdRRRDXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Author-contribution statements and acknowledgements in research papers should state clearly and specifically\n",
        "whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and\n",
        "analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts\n",
        "more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should\n",
        "be transparent about their use of LLMs, for example when selecting submitted manuscripts.\n",
        "\n",
        "Mention the large language model based product mentioned in the paragraph above:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "bmjet351RHEQ",
        "outputId": "57165bfa-7923-4ce7-81b6-1cf5526bce8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nThe large language model mentioned in the paragraph is ChatGPT."
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You can play with different variants of 4bit quantization such as NF4 (normalized float 4 (default))\n",
        "or pure FP4 quantization. Based on theoretical considerations and empirical results from the paper, we recommend using\n",
        "NF4 quantization for better performance.\n",
        "\n",
        "Based on the following paragraph, which quantization is better for performance? Only give me a one word answer.\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "Qa8ji1EIjS63",
        "outputId": "398dd49a-7d94-4dde-e233-03b4acfd804e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nNF4"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Answering\n",
        "\n",
        "Extension of information extraction; We can ask the model to answer specific questions based on prompt given. Q: A: format to be concise (or any variation of such). Combine instructions, context, input, and output indicators for better response results. Essentially: the better the question, the better the answer."
      ],
      "metadata": {
        "id": "vTh6tC6YQyLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not\n",
        "sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists\n",
        "generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind\n",
        "to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent\n",
        "organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "id": "KSjaJbBBQ1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "47ee8948-626e-4c9d-ec73-1285ec6e709f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mice"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets edit the prompt so we can get the model to respond that it is not sure about the answer."
      ],
      "metadata": {
        "id": "ute9N0WGLudF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not\n",
        "sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists\n",
        "generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind\n",
        "to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent\n",
        "organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: Who was the first patient that recieved a kidney transplant wiht the help of Teplizumab/OKT3?\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "WS7IRdQlmlxZ",
        "outputId": "19d37e93-c4f9-4ff6-9690-b14a52a87be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unsure about answer."
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification\n",
        "\n",
        "Categorize/labeling of given text. For example, sentiment analysis. Can make response to prompt better by providing examples for the model."
      ],
      "metadata": {
        "id": "5ZqslyrmnjIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative, or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "hBt5F0thoa3v",
        "outputId": "59866ba9-3bef-471f-a6bd-26a78528a14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Neutral"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Providing examples within the prompt for better responses.\n",
        "# In this instance, we will provide model some examples of text and sentiment pairs,\n",
        "# with the sentiments in a very specific format.\n",
        "prompt = \"\"\"Classify the text into neutral, negative, or positive.\n",
        "\n",
        "Text: The car show was 3 hours long.\n",
        "Sentiment: NEUTRaL\n",
        "\n",
        "Text: I failed the math exam today and it made me very unhappy.\n",
        "Sentiment: BaDDD!\n",
        "\n",
        "Text: Going to the beach today was so fun!\n",
        "Sentiment: GOooD.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "Sentiment:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "1eOpJv_hom7T",
        "outputId": "a3dd01f2-44bc-486b-c207-1502e8d50d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " NEUTRaL"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets modify the above prompt to instruct the model to give us an explanation to the answer.\n",
        "prompt = \"\"\"Classify the text into neutral, negative, or positive. Then, give me an explanation to the answer you\n",
        "selected (only the sentiment for the last text.)\n",
        "\n",
        "Text: The car show was 3 hours long.\n",
        "Sentiment: NEUTRaL\n",
        "\n",
        "Text: I failed the math exam today and it made me very unhappy.\n",
        "Sentiment: BaDDD!\n",
        "\n",
        "Text: Going to the beach today was so fun!\n",
        "Sentiment: GOooD.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "Sentiment:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yTdKmn5qrlFz",
        "outputId": "ec337b9c-788a-4201-a9f9-85d52eca9de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " NEUTRAL.\n\nExplanation: The sentiment for the last text is neutral because it does not express a strong opinion either way."
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Role Playing\n",
        "\n",
        "We can instruct the model on how to behave, respond, and give it the appearance of an identity. This can prove very useful for conversational systems like chatbots. Let the model role play as a 200IQ theoretical physiscist and its response can \"simulate\" such a role."
      ],
      "metadata": {
        "id": "3fi7TbfDsRQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical\n",
        "and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "KvJbLac2sQqG",
        "outputId": "3f495fe6-b7b0-4eeb-a9a9-2ea8732ee28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Sure! A black hole is a region of spacetime exhibiting gravitational acceleration so strong that nothing—no particles or even electromagnetic radiation such as light—can escape from it. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of the region from which no escape is possible is called the event horizon."
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets modify the prompt so that the response is concise.\n",
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is\n",
        "technical and scientific. The assistant's responses are to be concise and short.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "NQtAiDo5tHuW",
        "outputId": "e7f851ea-8a69-490c-b3aa-5fd96ae22f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Sure. A black hole is an object in space created when a very large star collapses in on itself due to its immense gravity. This collapse causes the star to become so dense that not even light can escape its gravitational pull."
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Generation\n",
        "\n",
        "LLMs, unsurprisingly, can output code as well. In fact, it is generally pretty good at it (if the specific model used has been sufficently trained on code data."
      ],
      "metadata": {
        "id": "aqKIiRlatYaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Given the following text, I want you to give me the equivalent python code.\n",
        "\n",
        "/* Ask the user for their name and say \"Hello\" */\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "C0jRGVqStwQY",
        "outputId": "c27dc6b1-11d5-446e-d398-41d956022921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nname = input(\"Please enter your name: \")\nprint(\"Hello \" + name)"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Table departments, columns = [DepartmentId, DepartmentName]\n",
        "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
        "Create a MySQL query for all students in the Computer Science Department\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "Q9IzxNq2vlPk",
        "outputId": "e3646d40-2ea7-4211-c02d-87ffca1f9cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSELECT StudentId, StudentName \nFROM students \nWHERE DepartmentId = (SELECT DepartmentId \n                      FROM departments \n                      WHERE DepartmentName = 'Computer Science');"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reasoning\n",
        "\n",
        "Very difficult for current LLMs to achieve currently (though possible, like in GPT-4, which shows some advanced reasoning skills). Such examples of reasoning includes solving mathemetical problems, puzzles, etc. Often needs advanced prompting techniques like Chain of Thought or self-consistency for better results."
      ],
      "metadata": {
        "id": "1YI6KiPUvx60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets start with a simple math problem that the model should solve.\n",
        "prompt = \"\"\" What is 10000 * 10000?\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "7mLfxEJ2vsiq",
        "outputId": "38641890-7e68-4c53-c299-1d01f7b6b266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n100000000"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now ask the model to solve a more difficult task (Expected answer is 41)."
      ],
      "metadata": {
        "id": "8aTyPSb-LWd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Give me the sum of the numbers in this group (whose sum will be an even number): 15, 32, 5, 13, 82, 7, 1.\n",
        "A: \"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "UPrbDQzXwy0_",
        "outputId": "b1466380-dca3-44f7-af85-4d44a52c7a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " 135"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell outputs an incorrect answer. Lets modify the prompt so that it can figure out the right answer."
      ],
      "metadata": {
        "id": "N_LhW-YrLMow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Give me the sum of the numbers in this group (whose sum will be an even number): 15, 32, 5, 13, 82, 7, 1.\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "xthcQ8P1xST2",
        "outputId": "98289e5b-a593-49a7-8ea8-90851e35468b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nOdd numbers: 15, 5, 13, 7, 1\nSum of odd numbers = 41 (odd)\n\nNow, add the even numbers:\n\nEven numbers: 32, 82\nSum of even numbers = 114 (even)\n\nFinally, add the sum of the odd numbers and the sum of the even numbers together:\n\n41 + 114 = 155 (odd)"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets improve this prompt even further for better results.\n",
        "prompt = \"\"\"Give me the sum of the numbers in this group (whose sum will be an even number): 15, 32, 5, 13, 82, 7, 1.\n",
        "Solve by breaking the problem into steps:\n",
        "Step 1. identify the odd numbers. This can be achieved by determining if each number is divisible by 2 evenly.\n",
        "If not, then it is a odd number.\n",
        "Step 2. After determining the odd numbers in the group, add all of them together.\n",
        "Step 3. Determine whether the sum of the odd numbers is odd or even. The sum of them should be odd. If not, then\n",
        "the answer is not correect.\n",
        "Complete each step and then give me your answer.\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "y3mzPRZmykYg",
        "outputId": "e05fe3a6-1f2a-406b-ebb8-a121459cf0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Answer: The sum of the odd numbers in the group is 27 (15 + 5 + 13 + 7 + 1). The sum is an odd number."
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the answer is still not correct. In this case, we can either use more advanced prompting techniques (which we are about to get into) in order to extract the full capabilities of the model itself, or alternatively, finetune the model on data that better elucidates reasoning (such as a math dataset), so that the model can answer the question more precisely, since in some cases prompting will simply not be enough."
      ],
      "metadata": {
        "id": "A3hpnijB128j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-Shot Prompting\n",
        "\n",
        "We have seen this before: prompting the model with some examples in order for the model to make better responses to the prompt. To expand on this, when one provides examples for the model, it enables in-context learning, in which the model is steered for better performance of the task based on the demonstrations provided.\n",
        "\n",
        "This is in contrast to zero-shot prompting, which does not involve and examples within the prompt."
      ],
      "metadata": {
        "id": "zOxGBsMR5CLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic few-shot example. Notice the lack on consistency in the format of the prompt.\n",
        "prompt = \"\"\"\n",
        "Positive This is awesome!\n",
        "This is bad! Negative\n",
        "Wow that movie was rad!\n",
        "Positive\n",
        "What a horrible show! --\n",
        "\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "WhP63FX-62IL",
        "outputId": "ae23fb06-cc4d-49bc-94f8-40e5a44ce41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Negative"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While it is better than zero-shot prompting at some tasks, it doesn't do as good a job for tasks that requires reasoning. For example:"
      ],
      "metadata": {
        "id": "zfNgsp_X7Gob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few shot example for more complex tasks.\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "IhleBFyN4e8Z",
        "outputId": "b5a9d2d6-c371-4b5b-c300-da2bcb6b1e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The answer is True."
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the answer (which is supposed to be False, 41), gives us True, which is incorrect. So we can create more sophisticated prompting techiques for better resilts."
      ],
      "metadata": {
        "id": "P_4-nNJk7euk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain of Thought Prompting (CoT)\n",
        "\n",
        "Enables better reasoning capabilities through intermediate reasoning steps."
      ],
      "metadata": {
        "id": "Dv3gXhns7y4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example One-shot CoT prompt.\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "qIOnRJSJEbZj",
        "outputId": "414f92bc-4524-4483-f51a-a58b62a619ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response in the above example is correct. This proves better than the few-shot example earilier where it returned True.\n",
        "\n",
        "Now, lets fuse CoT with few shot prompting."
      ],
      "metadata": {
        "id": "GSUZXlOMFGLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CoT + Few-shot.\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "_UWC9YN772pH",
        "outputId": "d842b746-6fcb-4639-a936-826835de81a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot CoT\n",
        "\n",
        "Addition of \"Let's think step by step\" to the prompt."
      ],
      "metadata": {
        "id": "P3xr_TPqF9oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the\n",
        "repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Q5bgH6NGGK5Z",
        "outputId": "fa6b5211-3505-45fe-b19c-93b7a27a364e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nI bought 10 apples: 10 apples \nI gave 2 apples to the neighbor and 2 to the repairman: 8 apples \nI bought 5 more apples: 13 apples \nI ate 1 apple: 12 apples \n\nTherefore, I remained with 12 apples."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Consistency\n",
        "\n",
        "Sampling of multiple reasoning paths via few-shot CoT. Selects the most consistent answer by marginalizing out the sampled reasoning paths."
      ],
      "metadata": {
        "id": "WPU-WW8WGoQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5\n",
        "\n",
        "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
        "A:\"\"\"\n",
        "\n",
        "outputs = []\n",
        "for i in range(10):\n",
        "  response = get_completion(params, prompt)\n",
        "  outputs.append(response)\n",
        "for i in range(len(outputs)):\n",
        "  print(\"Output\", i,\":\", outputs[i].choices[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgbSyHCgGndA",
        "outputId": "0c24d471-01e6-4ef8-c582-623a6b8d9f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output 0 :  When I was 6, my sister was half my age. That means she was 6/2 = 3 years old.\n",
            "Now I'm 70, so my sister must be 70/2 = 35 years old. The answer is 35.\n",
            "Output 1 :  When my sister was 6, I was 6. Now I'm 70, so my sister must be 70 / 2 = 35 years old. The answer is 35.\n",
            "Output 2 :  When you were 6, your sister was half your age. This means she was 6/2 = 3. Now that you are 70, your\n",
            "sister is 70/2 = 35. The answer is 35.\n",
            "Output 3 :  The sister was half the age of the person when he was 6 years old. So, when he was 6, his sister was 3.\n",
            "\n",
            "Now he is 70, so his sister is 70 - 3 = 67 years old. The answer is 67.\n",
            "Output 4 :  When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.\n",
            "Output 5 :  When you were 6, your sister was 3. Now you are 70, so your sister must be 70/2 = 35. The answer is 35.\n",
            "Output 6 :  When you were 6, your sister was half your age, so she was 3. Now you are 70, so your sister is 70/2 = 35. The answer is 35.\n",
            "Output 7 :  When you were 6, your sister was half your age. That means she was 6/2 = 3 years old.\n",
            "Now you are 70, so your sister is 70 - 3 = 67 years old. The answer is 67 years old.\n",
            "Output 8 :  When you were 6, your sister was 3 (half your age). 70 years later your sister is 73 (70 + 3). The answer is 73.\n",
            "Output 9 :  When I was 6 my sister was half my age, so she was 3. Now I'm 70, so she must be 70/2 = 35 years old. The answer is 35.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-consistency generates multiple reasoning answering pairs and aggregates via a majority vote to find the most conistent answer. We can either take the unweighted sum/majority vote over the answer (which answer is more frequent), or we can compute the unnormalized log probabilities of the output and then use the softmax to normalize them. We can then take the weighted sum as the weights on each answer. Lets demonstrate this:"
      ],
      "metadata": {
        "id": "Dm3ZElQGl5lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets first show the method of simply taking the unweighted sum/ majortiy vote over the answer.\n",
        "import numpy as np\n",
        "\n",
        "answers = []\n",
        "# Extracting the exact numerical answer from each of the outputs.\n",
        "for i in range(len(outputs)):\n",
        "  answers.append(int(outputs[i].choices[0].text[-3:].replace(\".\", \"\")))\n",
        "print(f\"Numerical answers from output: {answers}\")\n",
        "\n",
        "unique, counts = np.unique(answers, return_counts=True)\n",
        "index = np.argmax(counts)\n",
        "print(f\"Best answer: {unique[index]}\")"
      ],
      "metadata": {
        "id": "Fj6oifigl4Vk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "b14c7345-8478-4594-a35f-e68c0d7feb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d0fae7dc2603>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Extracting the exact numerical answer from each of the outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Numerical answers from output: {answers}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'ld'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated Knowledge Prompting\n",
        "\n",
        "Generating knowledge from a language model, then providing the knowledge as additional input when answering a question.\n"
      ],
      "metadata": {
        "id": "bkejoTXWY6Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Part of golf is trying to get a higher point total than the others. Yes or no?\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "id": "gSmEb9nLh8H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Input: Greece is larger than mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "\n",
        "Input: Glasses always fog up.\n",
        "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
        "\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
        "\n",
        "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
        "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
        "\n",
        "Input: A rock is the same size as a pebble.\n",
        "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
        "\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "Knowledge:\"\"\""
      ],
      "metadata": {
        "id": "58lUozAyiU9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Data\n",
        "\n",
        "Self-explanatory: make the LLM generate data via proper prompting."
      ],
      "metadata": {
        "id": "a5WlhC6CyhSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Produce ONLY 10 exemplars for sentiment analysis. Examples are categorized as either positive or negative. Produce 2 negative\n",
        "examples and 8 positive examples. Use this format for the examples:\n",
        "\n",
        "Q: <sentence>\n",
        "A: <sentiment>\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "hFWzcl9QyhDZ",
        "outputId": "020d78d5-fe8a-40c9-ca05-e3a923f043b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nQ: I just got a promotion at work!\nA: Positive\n\nQ: I'm so grateful for the help my friends and family gave me.\nA: Positive\n\nQ: I'm so disappointed that I failed the test.\nA: Negative\n\nQ: I've been so stressed lately.\nA: Negative\n\nQ: I'm so proud of the results I achieved.\nA: Positive\n\nQ: I'm so excited for the upcoming vacation.\nA: Positive\n\nQ: I can't believe how hard this project is.\nA: Negative\n\nQ: I'm so happy to have experienced this journey.\nA: Positive\n\nQ: I feel so overwhelmed with all this work.\nA: Negative\n\nQ: I'm so fortunate to have such great colleagues.\nA: Positive\n\nQ: I'm so discouraged by the lack of progress.\nA: Negative\n\nQ: I'm so content with all that I have accomplished.\nA: Positive"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"produce 3 wine reviews and label taste, flavor, aroma related token; present the result as a json file\n",
        ", in addition add the coordinate of each term for NER task\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "yzU8luJwzPDL",
        "outputId": "639f5b61-8ade-40a3-c192-462aa2113d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n{\n  \"review_1\": {\n    \"text\": \"This is a full-bodied, earthy red wine with notes of blackberry and oak. The tannins are well-balanced and the finish is robust.\",\n    \"tokens\": [\n      {\n        \"token\": \"full-bodied\",\n        \"taste\": true,\n        \"flavor\": false,\n        \"aroma\": false,\n        \"coord\": [0, 11]\n      },\n      {\n        \"token\": \"earthy\",\n        \"taste\": false,\n        \"flavor\": true,\n        \"aroma\": false,\n        \"coord\": [12, 18]\n      },\n      {\n        \"token\": \"blackberry\",\n        \"taste\": false,\n        \"flavor\": true,\n        \"aroma\": true,\n        \"coord\": [30, 39]\n      },\n      {\n        \"token\": \"oak\",\n        \"taste\": false,\n        \"flavor\": true,\n        \"aroma\": true,\n        \"coord\": [40, 43]\n      },\n     "
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PAL (Program-Aided Language Models): Code as Reasoning\n",
        "\n",
        "Uses the model to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as Python Interpreter. This is in contrast to advanced prompting techniques such as Chain of Thought, which produces the intermediate steps via free-form text.\n",
        "\n",
        "Lets develop an application that is able to take in some data and answer questions about the data input, ie. reason about the question being asked through code."
      ],
      "metadata": {
        "id": "LGFi-B4Ez92v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of an instance of the text-davinci-003 model.\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)"
      ],
      "metadata": {
        "id": "GQRIsT3p1wV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question in which to ask the model.\n",
        "question = \"Which is the oldest penquin?\""
      ],
      "metadata": {
        "id": "WDm0VMAzFtsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell consists of the prompt in which to give the model. It contains examples of prompts and its subsequent reasoning steps, which are in the form of code.\n",
        "\n",
        "For example, the first example prompt (which we will call sub-prompts) shows a few penquins' name, age, height, and weight. It then asks \"How many penquins are less than 8 years old\"? This is followed by the reasoning steps, which puts the penguins in a python list, adds a fourth penguin, and then find the number of penguins less than 8 years old.\n",
        "\n",
        "The intuition is that the model will follow the logic of example code, and when given a new fresh prompt, it will follow the same reasoning steps to find the novel answer."
      ],
      "metadata": {
        "id": "cjaX578aHSOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "We now add a penguin to the table:\n",
        "James, 12, 90, 12\n",
        "How many penguins are less than 8 years old?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Add penguin James.\n",
        "penguins.append(('James', 12, 90, 12))\n",
        "# Find penguins under 8 years old.\n",
        "penguins_under_8_years_old = [penguin for penguin in penguins if penguin[1] < 8]\n",
        "# Count number of penguins under 8.\n",
        "num_penguin_under_8 = len(penguins_under_8_years_old)\n",
        "answer = num_penguin_under_8\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "Which is the youngest penguin?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort the penguins by age.\n",
        "penguins = sorted(penguins, key=lambda x: x[1])\n",
        "# Get the youngest penguin's name.\n",
        "youngest_penguin_name = penguins[0][0]\n",
        "answer = youngest_penguin_name\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "What is the name of the second penguin sorted by alphabetic order?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort penguins by alphabetic order.\n",
        "penguins_alphabetic = sorted(penguins, key=lambda x: x[0])\n",
        "# Get the second penguin sorted by alphabetic order.\n",
        "second_penguin_name = penguins_alphabetic[1][0]\n",
        "answer = second_penguin_name\n",
        "\"\"\"\n",
        "{question}\n",
        "\"\"\"\n",
        "'''.strip() + \"\\n\""
      ],
      "metadata": {
        "id": "1N96Nlw0FxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output = llm(prompt.format(question=question))\n",
        "print(llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3mJF68eGdq7",
        "outputId": "0c17751e-6b17-496d-8bf4-9d366d4bffe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Put the penguins into a list.\n",
            "penguins = []\n",
            "penguins.append(('Louis', 7, 50, 11))\n",
            "penguins.append(('Bernard', 5, 80, 13))\n",
            "penguins.append(('Vincent', 9, 60, 11))\n",
            "penguins.append(('Gwen', 8, 70, 15))\n",
            "# Sort the penguins by age.\n",
            "penguins = sorted(penguins, key=lambda x: x[1], reverse=True)\n",
            "# Get the oldest penguin's name.\n",
            "oldest_penguin_name = penguins[0][0]\n",
            "answer = oldest_penguin_name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exec(llm_output)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHshjoYTG3fA",
        "outputId": "05630821-b6a8-4dae-a88c-9752246edd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vincent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now try a different question and see the results."
      ],
      "metadata": {
        "id": "Ms8S7pk4I5qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which is the second youngest penquin?\"\n",
        "llm_output = llm(prompt.format(question=question))\n",
        "print(llm_output)\n",
        "\n",
        "exec(llm_output)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-5rW4JHI-tp",
        "outputId": "88699246-f726-4c02-cca8-d20eab9966aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Put the penguins into a list.\n",
            "penguins = []\n",
            "penguins.append(('Louis', 7, 50, 11))\n",
            "penguins.append(('Bernard', 5, 80, 13))\n",
            "penguins.append(('Vincent', 9, 60, 11))\n",
            "penguins.append(('Gwen', 8, 70, 15))\n",
            "# Sort the penguins by age.\n",
            "penguins = sorted(penguins, key=lambda x: x[1])\n",
            "# Get the second youngest penguin's name.\n",
            "second_youngest_penguin_name = penguins[1][0]\n",
            "answer = second_youngest_penguin_name\n",
            "Louis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Louis is indeed the second youngest penquin's name.\n",
        "\n",
        "Lets try another example:\n",
        "\n"
      ],
      "metadata": {
        "id": "PR4TNBDqJ5A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name='text-davinci-003', temperature=0)"
      ],
      "metadata": {
        "id": "_2-CB6V1J-pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Today is 27 Feburary 2023. I was born exactly 25 years ago. What is the date I was born in MM/DD/YYYY?\""
      ],
      "metadata": {
        "id": "FKBULQGRKVeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATE_UNDERSTANDING_PROMPT = \"\"\"\n",
        "# Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
        "# If 2015 is coming in 36 hours, then today is 36 hours before.\n",
        "today = datetime(2015, 1, 1) - relativedelta(hours=36)\n",
        "# One week from today,\n",
        "one_week_from_today = today + relativedelta(weeks=1)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "one_week_from_today.strftime('%m/%d/%Y')\n",
        "# Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?\n",
        "# If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.\n",
        "today = datetime(2019, 1, 1) + relativedelta(days=6)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "today.strftime('%m/%d/%Y')\n",
        "# Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\n",
        "# If the concert was scheduled to be on 06/01/1943, but was delayed by one day to today, then today is one day later.\n",
        "today = datetime(1943, 6, 1) + relativedelta(days=1)\n",
        "# 10 days ago,\n",
        "ten_days_ago = today - relativedelta(days=10)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "ten_days_ago.strftime('%m/%d/%Y')\n",
        "# Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
        "# It is 4/19/1969 today.\n",
        "today = datetime(1969, 4, 19)\n",
        "# 24 hours later,\n",
        "later = today + relativedelta(hours=24)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "today.strftime('%m/%d/%Y')\n",
        "# Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\n",
        "# If Jane thought today is 3/11/2002, but today is in fact Mar 12, then today is 3/1/2002.\n",
        "today = datetime(2002, 3, 12)\n",
        "# 24 hours later,\n",
        "later = today + relativedelta(hours=24)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "later.strftime('%m/%d/%Y')\n",
        "# Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\n",
        "# If Jane was born on the last day of Feburary in 2001 and today is her 16-year-old birthday, then today is 16 years later.\n",
        "today = datetime(2001, 2, 28) + relativedelta(years=16)\n",
        "# Yesterday,\n",
        "yesterday = today - relativedelta(days=1)\n",
        "# The answer formatted with %m/%d/%Y is\n",
        "yesterday.strftime('%m/%d/%Y')\n",
        "# Q: {question}\n",
        "\"\"\".strip() + '\\n'"
      ],
      "metadata": {
        "id": "rH4eWpg5KgbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output = llm(DATE_UNDERSTANDING_PROMPT.format(question=question))\n",
        "print(llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HlkUGRGKUJ4",
        "outputId": "d79e02f5-7580-43d8-b5c9-cbb993991692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# If today is 27 Feburary 2023 and I was born exactly 25 years ago, then I was born 25 years before.\n",
            "today = datetime(2023, 2, 27)\n",
            "# I was born,\n",
            "born = today - relativedelta(years=25)\n",
            "# The answer formatted with %m/%d/%Y is\n",
            "born.strftime('%m/%d/%Y')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "exec(llm_output)\n",
        "print(born)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YAPaXFMK6NW",
        "outputId": "eddf66a8-bfa2-4b36-d0a2-cdf8b8c78c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1998-02-27 00:00:00\n"
          ]
        }
      ]
    }
  ]
}